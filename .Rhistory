arrange(-n) %>%
View()
library(SnowballC)
# truncating
stems <- ukraine %>%
mutate(token_stem = wordStem(.$tokens, language = "german")) %>%
count(token_stem, sort = T)
stems
tibble(stems)
View(stems)
wordcloud::wordcloud(words = stems$token_stem, freq = stems$n)
library(ggplot2)
stems %>%
ggplot(aes(x = token_stem, y = n))+
geom_col()
stems %>%
head(20) %>%
ggplot(aes(x = token_stem, y = n))+
geom_col()
stems %>%
head(20) %>%
ggplot(aes(x = fct_reorder(token_stem, n), y = n))+
geom_col()
library(forcats)
stems %>%
head(20) %>%
ggplot(aes(x = reorder(token_stem, n), y = n))+
geom_col()
stems %>%
head(20) %>%
ggplot(aes(x = reorder(token_stem, n), y = n))+
geom_col()+
coord_flip()
?reorder
wordcloud::wordcloud(
words = stems$token_stem,
freq = stems$n
)
stems %>%
head(20) %>%
ggplot(aes(x = reorder(token_stem, n), y = n))+
geom_col()+
coord_flip()
stems %>%
head(30) %>%
ggplot(aes(x = reorder(token_stem, n), y = n))+
geom_col()+
coord_flip()
stems %>%
head(20) %>%
ggplot(aes(x = reorder(token_stem, n), y = n))+
geom_col()+
coord_flip()
View(articles)
#! /usr/bin/env Rscript
# Script to scrape news articles & metadata from welt.de. Excludes sponsored
# embedded advertisement posts and some other rubrics.
library(dplyr)
library(rvest)
library(purrr)
library(xml2)
url <- "https://www.welt.de/"
articles <-
url %>%
read_html() %>%
html_elements(".o-teaser__link--is-headline")
title <- xml_attr(articles, "title")
link <- xml_attr(articles, "href")
articles <- as.data.frame(cbind(title, link))
exclude <- c("TRUTH FACTS", "Die besten Bilder des Tages")
articles <-
articles %>%
filter(!grepl("sponsored", link) & !title %in% exclude) %>%
mutate(link = paste0("https://www.welt.de", link))
#----
html <- rvest::read_html(articles$link[[1]])
xml_names <- html %>%
html_elements("meta") %>%
xml_attr("name")
xml_content <- html %>%
html_elements("meta") %>%
xml_attr("content")
metas <- as.data.frame(cbind(xml_names, xml_content))
# works for description, news_keywords & date:
get_metadata <- function(url, query) {
html <- rvest::read_html(url)
xml_names <- html %>%
html_elements("meta") %>%
xml_attr("name")
xml_content <- html %>%
html_elements("meta") %>%
xml_attr("content")
metas <- as.data.frame(cbind(xml_names, xml_content))
return(
metas %>%
filter(xml_names == query) %>%
pull(xml_content)
)
}
# author is specially embedded:
get_author <- function(url) {
html <- rvest::read_html(url)
return(
html %>%
html_elements(".c-author__link") %>%
html_text2() %>%
paste0(., collapse = ", ")
)
}
get_body <- function(url) {
html <- rvest::read_html(url)
return(
html %>%
html_elements("p") %>%
html_text2() %>%
head(-2) %>%
paste0(., collapse = " ")
)
}
check_paywall <- function(url) {
pw <-
rvest::read_html(url) %>%
html_text() %>%
stringr::str_extract("isAccessibleForFree(.*?),")
if (grepl("False", pw)) return(1L) else return(0L)
}
# check paywall & retrieve metadata:
articles <-
articles %>%
mutate(
author = map_chr(link, get_author),
description = map_chr(link, ~ get_metadata(.x, "description")),
keywords = map_chr(link, ~ get_metadata(.x, "news_keywords")),
date = map_chr(link, ~ get_metadata(.x, "date")),
paywall = map_int(link, check_paywall),
timestamp = Sys.time(),
body = NA_character_
)
#! /usr/bin/env Rscript
# Retrieves all article metadata & bodies from the FAZ website.
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(dplyr)
library(tidyr)
library(rvest)
library(xml2)
url <- "https://www.faz.net/aktuell/"
faz <- url %>% read_html()
articles <- faz %>% html_elements(".tsr-Base_ContentLink")
title <- articles %>% xml_attr("title")
link <- articles %>% xml_attr("href")
paywall <- articles %>% xml_attr("data-is-premium")
articles <-
cbind(title, link, paywall) %>%
as.data.frame() %>%
mutate(
paywall = case_when(
paywall == "true" ~ 1L,
paywall == "false" ~ 0L,
TRUE ~ NA_integer_
)
) %>%
# Try to drop embedded ads, podcasts & duplicates:
filter(grepl("https://www.faz.net/", link) | is.na(link)) %>%
filter(!grepl("[Ii]m Test:", title) & !grepl("^F\\.A\\.Z\\.", title)) %>%
distinct()
get_description <- function(src) {
out <-
src %>%
html_elements(xpath = '//*[@name="description"]') %>%
xml_attr("content")
if (rlang::is_empty(out)) out <- NA_character_
return(out)
}
get_keywords <- function(src) {
out <-
src %>%
html_elements(xpath = '//*[@name="keywords"]') %>%
xml_attr("content")
# Case of no assigned keywords:
if (rlang::is_empty(out)) out <- NA_character_
return(out)
}
get_author <- function(src) {
out <-
src %>%
html_elements(".aut-Teaser") %>%
html_attr("data-author-name")
# Case of no author:
if (rlang::is_empty(out)) {
out <-
src %>%
html_text2() %>%
stringr::str_extract('"author":\\{"@type":"Person","name":"(.*?)",') %>%
stringr::str_remove('.*name":"') %>%
stringr::str_remove('",$')
}
# Case of multiple authors:
if (length(out) > 1) {
out <- paste(out, collapse = ", ")
}
return(out)
}
get_publication_date <- function(src) {
out <-
src %>%
html_text2() %>%
stringr::str_extract('first-publication":"(.*?)"') %>%
stringr::str_remove('.*:\"') %>%
stringr::str_remove('\"$')
if (rlang::is_empty(out)) out <- NA_character_
return(out)
}
get_body <- function(src) {
src %>%
html_text2() %>%
stringr::str_extract('"articleBody":"(.*)\",\"dateP') %>%
mgsub::mgsub(
c('.*:\"', '\",\"dateP$'),
rep("", 2)
)
}
#--------------------------------------------------------------------
# Pre-allocate output:
articles <-
articles %>%
mutate(
description = NA_character_,
keywords    = NA_character_,
author      = NA_character_,
date        = NA_character_,
body        = NA_character_
)
for (i in seq_along(articles$link)) {
print(paste0("Retrieving ", i, "/", length(articles$link)))
if (is.na(articles$link[[i]])) next
page_src <- rvest::read_html(articles$link[[i]])
articles$description[[i]] <- get_description(page_src)
articles$keywords[[i]]    <- get_keywords(page_src)
articles$author[[i]]      <- get_author(page_src)
articles$date[[i]]        <- get_publication_date(page_src)
if (articles$paywall[[i]] == 0) {
articles$body[[i]] <- get_body(page_src)
}
}
# Add timestamp:
articles <- articles %>% mutate(timestamp = Sys.time())
# Export:
time <-
Sys.time() %>%
as.character() %>%
stringr::str_replace(" ", "_")
name <- paste0("faz_", time, ".csv")
write.csv(articles, paste0("../data/", name))
write.csv(articles, name)
#! /usr/bin/env Rscript
# Retrieve article metadata & bodies from the ZEIT website.
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(dplyr)
library(tidyr)
library(rvest)
library(xml2)
url <- "https://www.zeit.de/index"
exclude <- c(
"Direkt zum Inhalt springen", "exklusive Zeit Artikel",
"Nachrichten auf ZEIT ONLINE", "Kommentare anzeigen",
"ZurÃ¼ck nach oben", "ZEIT ONLINE Stimmen: Hier kommen Sie zu Wort"
)
page <-
url %>%
httr::GET(url = .) %>%
rvest::read_html()
title <-
page %>%
html_elements("article") %>%
html_elements("a") %>%
xml_find_all('//*[@title and @href]') %>%
xml_attr("title")
link <-
page %>%
html_elements("article") %>%
html_elements("a") %>%
xml_find_all('//*[@title and @href]') %>%
xml_attr("href")
articles <-
as.data.frame(cbind(title, link)) %>%
filter(!title %in% exclude & grepl("https://www.zeit.de/", link)) %>%
distinct()
# Retrieving metadata & body:
get_description <- function(src) {
out <-
src %>%
html_elements(xpath = '//*[@name="description"]') %>%
xml_attr("content")
if (rlang::is_empty(out)) out <- NA_character_
return(out)
}
get_keywords <- function(src) {
out <-
src %>%
html_elements(xpath = '//*[@name="keywords"]') %>%
xml_attr("content")
if (rlang::is_empty(out)) out <- NA_character_
return(out)
}
get_publication_date <- function(src) {
out <-
src %>%
html_elements(xpath = '//*[@name="date"]') %>%
xml_attr("content")
if (rlang::is_empty(out)) out <- NA_character_
return(out)
}
get_author <- function(src) {
out <-
src %>%
html_elements(xpath = '//*[@rel="author"]') %>%
html_text2()
# If no author:
if (rlang::is_empty(out)) out <- NA_character_
# If multiple authors:
if (length(out) > 1) out <- paste(out, collapse = ", ")
return(out)
}
get_paywall <- function(src) {
if (grepl('"paywall": "open"', src))
return(0L)
else
return(1L)
}
get_body <- function(src) {
out <-
src %>%
html_elements(xpath = '//*[@class="paragraph article__item"]') %>%
html_text2() %>%
paste(collapse = "")
if (rlang::is_empty(out)) out <- NA_character_
return(out)
}
test <- httr::GET(articles$link[[1]]) %>% rvest::read_html()
# Pre-allocate output:
articles <-
articles %>%
mutate(
description = NA_character_,
keywords    = NA_character_,
date        = NA_character_,
author      = NA_character_,
paywall     = NA_character_,
body        = NA_character_
)
for (i in seq_along(articles$link)) {
print(paste0("Retrieving ", i, "/", length(articles$link)))
page_src <- httr::GET(articles$link[[i]]) %>% rvest::read_html()
articles$description[[i]] <- get_description(page_src)
articles$keywords[[i]]    <- get_keywords(page_src)
articles$date[[i]]        <- get_publication_date(page_src)
articles$author[[i]]      <- get_author(page_src)
articles$paywall[[i]]     <- get_paywall(page_src)
if (articles$paywall[[i]] == 0) {
articles$body[[i]] <- get_body(page_src)
}
}
# Add timestamp:
articles <- articles %>% mutate(timestamp = Sys.time())
# Export:
time <-
Sys.time() %>%
as.character() %>%
stringr::str_replace(" ", "_")
name <- paste0("zeit_", time, ".csv")
write.csv(articles, paste0("../data/", name))
tibble(articles)
write.csv(articles, name)
library(rvest)
library(dplyr)
library(xml2)
library(XML)
library(purrr)
src <- read_html("https://www.welt.de/sitemaps/newssitemap/newssitemap.xml")
nodes <- src %>% html_elements("url")
safe_extract <- function(node, keyword) {
out <- node %>%
html_elements(keyword) %>%
html_text2()
if (rlang::is_empty(out)) out <- NA_character_
return(out)
}
contents <- lapply(
nodes,
function(x) {
y <- html_elements(x, "news")
data.frame(
keywords = safe_extract(y, "keywords"),
date = safe_extract(y, "publication_date"),
title = safe_extract(y, "title"),
url = x %>%
html_element("loc") %>%
html_text2()
)
}
)
articles <- tibble(do.call(rbind, contents))
check_paywall <- function(x) {
pw <- x %>%
html_text() %>%
stringr::str_extract("isAccessibleForFree(.*?),")
if (grepl("False", pw)) return(1L) else return(0L)
}
pb <- progress::progress_bar$new()
articles2 <- lapply(
articles$url,
function(url) {
pb$tick()
html <- read_html(url)
out <- data.frame(
author = html %>%
html_elements(".c-author__link") %>%
html_text2() %>%
paste0(., collapse = ", "),
description = html %>%
html_elements(xpath = '//*[@name="description"]') %>%
xml_attr("content"),
paywall = check_paywall(html)
)
out <- out %>%
mutate(
body = ifelse(
paywall == 0,
html %>%
html_elements("p") %>%
html_text2() %>%
head(-2) %>%
paste0(., collapse = " "),
NA_character_
)
)
return(out)
}
)
pb$terminate()
articles2 <- tibble(do.call(rbind, articles2))
# WELT.de with sitemaps:
library(rvest)
library(dplyr)
library(xml2)
library(XML)
library(purrr)
src <- read_html("https://www.welt.de/sitemaps/newssitemap/newssitemap.xml")
nodes <- src %>% html_elements("url")
safe_extract <- function(node, keyword) {
out <- node %>%
html_elements(keyword) %>%
html_text2()
if (rlang::is_empty(out)) out <- NA_character_
return(out)
}
contents <- lapply(
nodes,
function(x) {
y <- html_elements(x, "news")
data.frame(
keywords = safe_extract(y, "keywords"),
date = safe_extract(y, "publication_date"),
title = safe_extract(y, "title"),
url = x %>%
html_element("loc") %>%
html_text2()
)
}
)
articles <- tibble(do.call(rbind, contents))
check_paywall <- function(x) {
pw <- x %>%
html_text() %>%
stringr::str_extract("isAccessibleForFree(.*?),")
if (grepl("False", pw)) return(1L) else return(0L)
}
pb <- progress::progress_bar$new(total = length(articles$url))
articles2 <- lapply(
articles$url,
function(url) {
pb$tick()
html <- read_html(url)
out <- data.frame(
author = html %>%
html_elements(".c-author__link") %>%
html_text2() %>%
paste0(., collapse = ", "),
description = html %>%
html_elements(xpath = '//*[@name="description"]') %>%
xml_attr("content"),
paywall = check_paywall(html)
)
out <- out %>%
mutate(
body = ifelse(
paywall == 0,
html %>%
html_elements("p") %>%
html_text2() %>%
head(-2) %>%
paste0(., collapse = " "),
NA_character_
)
)
return(out)
}
)
articles2 <- tibble(do.call(rbind, articles2))
articles <- tibble(cbind(articles, articles2))
tibble(articles)
View(articles)
